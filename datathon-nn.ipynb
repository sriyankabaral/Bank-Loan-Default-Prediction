{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91018,"databundleVersionId":10698415,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:44:25.300784Z","iopub.execute_input":"2025-01-06T17:44:25.301287Z","iopub.status.idle":"2025-01-06T17:44:25.766230Z","shell.execute_reply.started":"2025-01-06T17:44:25.301248Z","shell.execute_reply":"2025-01-06T17:44:25.765168Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/techparva3-datathon/sample_submission.csv\n/kaggle/input/techparva3-datathon/train.csv\n/kaggle/input/techparva3-datathon/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import Necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data Preprocessing and Feature Engineering\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.impute import KNNImputer\nfrom category_encoders import TargetEncoder\nfrom imblearn.combine import SMOTEENN\n\n# Feature Selection\nfrom sklearn.feature_selection import RFECV\n\n# Model Selection\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\n# Evaluation Metrics\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n\n# Suppress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Step 1: Load and Inspect Data\n# Replace the path with your actual data path\ndata = pd.read_csv('/kaggle/input/techparva3-datathon/train.csv')\n\n# Display basic information\nprint(\"Initial Data Shape:\", data.shape)\nprint(\"Data Columns:\", data.columns.tolist())\nprint(\"\\nMissing Values:\\n\", data.isnull().sum())\n\n# Step 2: Data Cleaning and Preprocessing\ndef clean_and_preprocess_data(df):\n    # 2.1 Drop Unnecessary Columns\n    columns_to_drop = [\n        'application_type',  # Assuming only 'INDIVIDUAL' is present\n        'emp_title',         # High-cardinality; drop or process if meaningful\n        'member_id',         # Unique identifier; drop to prevent data leakage\n        'issue_date',        # Date; can extract features if needed\n        'last_credit_pull_date',\n        'last_payment_date',\n        'next_payment_date'\n    ]\n    df = df.drop(columns=columns_to_drop, errors='ignore')\n    \n    # 2.2 Convert 'emp_length' to Numeric Before Imputation\n    def convert_emp_length(emp_length):\n        if isinstance(emp_length, str):\n            if '< 1 year' in emp_length:\n                return 0.0\n            elif '10+ years' in emp_length:\n                return 10.0\n            else:\n                try:\n                    return float(emp_length.split(' ')[0])\n                except ValueError:\n                    return np.nan\n        return np.nan\n    \n    if 'emp_length' in df.columns:\n        df['emp_length_num'] = df['emp_length'].apply(convert_emp_length)\n        df = df.drop(columns=['emp_length'], errors='ignore')\n    \n    # 2.3 Handle Missing Values with KNN Imputer\n    imputer = KNNImputer(n_neighbors=5)\n    numerical_cols = [\n        'annual_income', 'dti', 'installment', 'int_rate', \n        'loan_amount', 'total_acc', 'total_payment', 'emp_length_num'\n    ]\n    \n    # Check if 'emp_length_num' exists before imputation\n    existing_numerical_cols = [col for col in numerical_cols if col in df.columns]\n    \n    df[existing_numerical_cols] = imputer.fit_transform(df[existing_numerical_cols])\n    \n    # 2.4 Remove Duplicates\n    initial_shape = df.shape\n    df = df.drop_duplicates()\n    final_shape = df.shape\n    print(f\"Removed {initial_shape[0] - final_shape[0]} duplicate rows.\")\n    \n    # 2.5 Handle Outliers Using the IQR Method\n    for col in existing_numerical_cols:\n        if col in df.columns:\n            Q1 = df[col].quantile(0.25)\n            Q3 = df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            initial_shape = df.shape\n            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n            final_shape = df.shape\n            print(f\"Removed {initial_shape[0] - final_shape[0]} outliers from '{col}'.\")\n    \n    # 2.6 Feature Engineering: Interest-to-Loan Ratio\n    if 'int_rate' in df.columns and 'loan_amount' in df.columns:\n        df['interest_to_loan_ratio'] = df['int_rate'] / (df['loan_amount'] + 1e-5)  # Avoid division by zero\n    \n    return df\n\n# Apply Cleaning and Preprocessing\ndata = clean_and_preprocess_data(data)\nprint(\"\\nData Shape after Cleaning:\", data.shape)\nprint(\"\\nMissing Values after Imputation:\\n\", data.isnull().sum())\n\n# Step 3: Encode Target Variable\n# Assuming 'grade' is the target variable\nlabel_encoder = LabelEncoder()\ndata['grade_encoded'] = label_encoder.fit_transform(data['grade'])\n\n# Step 4: Define Features (X) and Target (y)\nX = data.drop(columns=['grade', 'grade_encoded'], errors='ignore')\ny = data['grade_encoded']\n\n# Step 5: Advanced Encoding for Categorical Variables\ncategorical_cols = ['address_state', 'home_ownership', 'loan_status', 'purpose', 'term', 'verification_status']\n# Note: 'loan_status' can have categories like 'Current', 'Charged Off', etc.\n\n# Initialize Target Encoder\ntarget_encoder = TargetEncoder(cols=categorical_cols)\nX[categorical_cols] = target_encoder.fit_transform(X[categorical_cols], y)\n\n# Step 6: Feature Selection - Remove Highly Correlated Features\ndef remove_correlated_features(df, threshold=0.9):\n    corr_matrix = df.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    df = df.drop(columns=to_drop)\n    return df, to_drop\n\nX, dropped_features = remove_correlated_features(X, threshold=0.9)\nprint(f\"\\nDropped {len(dropped_features)} highly correlated features: {dropped_features}\")\n\n# Step 7: Create Interaction Features (Optional)\n# Example: Creating a new feature by multiplying 'annual_income' and 'dti'\nif 'annual_income' in X.columns and 'dti' in X.columns:\n    X['income_dti'] = X['annual_income'] * X['dti']\n\n# Step 8: Split the Data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(f\"\\nTraining Samples: {X_train.shape[0]}, Testing Samples: {X_test.shape[0]}\")\n\n# Step 9: Handle Class Imbalance with SMOTEENN\nsmote_enn = SMOTEENN(random_state=42)\nX_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train)\nprint(f\"Original Training Samples: {X_train.shape[0]}, Resampled Training Samples: {X_train_resampled.shape[0]}\")\n\n# Step 10: Feature Scaling with RobustScaler\nscaler = RobustScaler()\nX_train_resampled = scaler.fit_transform(X_train_resampled)\nX_test = scaler.transform(X_test)\n\n# Step 11: Feature Selection with RFECV (Recursive Feature Elimination with Cross-Validation)\nrf_estimator = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrfecv = RFECV(\n    estimator=rf_estimator,\n    step=10,\n    cv=StratifiedKFold(5),\n    scoring='accuracy',\n    n_jobs=-1\n)\nrfecv.fit(X_train_resampled, y_train_resampled)\nprint(f\"\\nOptimal number of features: {rfecv.n_features_}\")\n\n# Select Features\nX_train_selected = rfecv.transform(X_train_resampled)\nX_test_selected = rfecv.transform(X_test)\n\n# Step 12: Model Training with XGBoost and Hyperparameter Tuning\nxgb_model = XGBClassifier(\n    objective='multi:softprob',\n    num_class=len(np.unique(y)),\n    eval_metric='mlogloss',\n    use_label_encoder=False,\n    random_state=42\n)\n\n# Define Hyperparameter Grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [6, 10, 15],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.7, 0.8, 1.0],\n    'colsample_bytree': [0.7, 0.8, 1.0],\n    'gamma': [0, 0.1, 0.2],\n    'reg_alpha': [0, 0.1, 1],\n    'reg_lambda': [1, 1.5, 2]\n}\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,\n    verbose=2,\n    n_jobs=-1\n)\n\n# Fit GridSearchCV\nprint(\"\\nStarting Grid Search for XGBoost...\")\ngrid_search.fit(X_train_selected, y_train_resampled)\n\n# Best Estimator\nbest_xgb = grid_search.best_estimator_\nprint(f\"\\nBest XGBoost Parameters: {grid_search.best_params_}\")\n\n# Step 13: Evaluate the Best Model\ny_pred = best_xgb.predict(X_test_selected)\n\n# Accuracy Score\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nXGBoost Accuracy: {accuracy:.4f}\")\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=label_encoder.classes_, \n            yticklabels=label_encoder.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix - XGBoost')\nplt.show()\n\n# Step 14: Cross-Validation for Robust Evaluation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(best_xgb, X_train_selected, y_train_resampled, cv=cv, scoring='accuracy', n_jobs=-1)\nprint(f\"\\nCross-Validation Accuracy Scores: {cv_scores}\")\nprint(f\"Mean CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n\n# Step 15: Feature Importance Visualization\nimportances = best_xgb.feature_importances_\n# Retrieve feature names from RFECV\nif hasattr(rfecv, 'get_feature_names_out'):\n    feature_names = rfecv.get_feature_names_out()\nelse:\n    # If not available, create generic feature names\n    feature_names = [f'Feature {i}' for i in range(importances.shape[0])]\n\nfeature_importance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importances\n}).sort_values(by='importance', ascending=False)\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x='importance', y='feature', data=feature_importance_df.head(20))\nplt.title('Top 20 Feature Importances - XGBoost')\nplt.xlabel('Importance Score')\nplt.ylabel('Feature')\nplt.show()\n\n# Optional: Ensemble with Voting Classifier\n# Initialize Random Forest for Ensemble\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=10,\n    random_state=42,\n    class_weight='balanced'\n)\n\n# Initialize Voting Classifier with XGBoost and Random Forest\nvoting_clf = VotingClassifier(\n    estimators=[('xgb', best_xgb), ('rf', rf_model)],\n    voting='soft',\n    n_jobs=-1\n)\n\n# Fit Voting Classifier\nprint(\"\\nTraining Voting Classifier...\")\nvoting_clf.fit(X_train_selected, y_train_resampled)\n\n# Predict with Voting Classifier\ny_pred_voting = voting_clf.predict(X_test_selected)\n\n# Evaluate Voting Classifier\naccuracy_voting = accuracy_score(y_test, y_pred_voting)\nprint(f\"\\nVoting Classifier Accuracy: {accuracy_voting:.4f}\")\nprint(\"\\nVoting Classifier Classification Report:\")\nprint(classification_report(y_test, y_pred_voting, target_names=label_encoder.classes_))\n\n# Confusion Matrix for Voting Classifier\ncm_voting = confusion_matrix(y_test, y_pred_voting)\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm_voting, annot=True, fmt='d', cmap='Greens', \n            xticklabels=label_encoder.classes_, \n            yticklabels=label_encoder.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix - Voting Classifier')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:25:46.375567Z","iopub.execute_input":"2025-01-06T18:25:46.375933Z"}},"outputs":[{"name":"stdout","text":"Initial Data Shape: (75001, 27)\nData Columns: ['address_state', 'application_type', 'emp_length', 'emp_title', 'grade', 'home_ownership', 'issue_date', 'last_credit_pull_date', 'last_payment_date', 'loan_status', 'next_payment_date', 'member_id', 'purpose', 'term', 'verification_status', 'annual_income', 'dti', 'installment', 'int_rate', 'loan_amount', 'total_acc', 'total_payment', 'issue_date_year', 'issue_date_month', 'issue_date_day', 'issue_date_weekday', 'issue_date_hour']\n\nMissing Values:\n address_state                0\napplication_type             0\nemp_length                   0\nemp_title                    0\ngrade                        0\nhome_ownership               0\nissue_date                   0\nlast_credit_pull_date        0\nlast_payment_date            0\nloan_status                  0\nnext_payment_date            0\nmember_id                    0\npurpose                      0\nterm                         0\nverification_status          0\nannual_income             7500\ndti                      11250\ninstallment               3750\nint_rate                 15000\nloan_amount               7500\ntotal_acc                 7500\ntotal_payment            18750\nissue_date_year              0\nissue_date_month             0\nissue_date_day               0\nissue_date_weekday           0\nissue_date_hour              0\ndtype: int64\nRemoved 0 duplicate rows.\nRemoved 1642 outliers from 'annual_income'.\nRemoved 812 outliers from 'dti'.\nRemoved 900 outliers from 'installment'.\nRemoved 652 outliers from 'int_rate'.\nRemoved 881 outliers from 'loan_amount'.\nRemoved 1148 outliers from 'total_acc'.\nRemoved 1269 outliers from 'total_payment'.\nRemoved 0 outliers from 'emp_length_num'.\n\nData Shape after Cleaning: (67697, 21)\n\nMissing Values after Imputation:\n address_state             0\ngrade                     0\nhome_ownership            0\nloan_status               0\npurpose                   0\nterm                      0\nverification_status       0\nannual_income             0\ndti                       0\ninstallment               0\nint_rate                  0\nloan_amount               0\ntotal_acc                 0\ntotal_payment             0\nissue_date_year           0\nissue_date_month          0\nissue_date_day            0\nissue_date_weekday        0\nissue_date_hour           0\nemp_length_num            0\ninterest_to_loan_ratio    0\ndtype: int64\n\nDropped 0 highly correlated features: []\n\nTraining Samples: 54157, Testing Samples: 13540\nOriginal Training Samples: 54157, Resampled Training Samples: 1766\n\nOptimal number of features: 11\n\nStarting Grid Search for XGBoost...\nFitting 3 folds for each of 6561 candidates, totalling 19683 fits\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}